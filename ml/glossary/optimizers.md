## Оптимизаторы

### Метод градиентного спуска (GD)
На каждой итерации вычисляем частные производные от целевой функции,
перемножаем на коэффициент скорости обучения и получаем значения для обновления весов.
Однако, слои в начале сети находятся далеко от слоев на выходе,
поэтому приходится считать большое количество производных,
чтобы учесть такие зависимости. Во первых, это довольно накладно,
а во-вторых, чревато застреваниями на плато и в локальных максимумах.


### Nesterov Accelerated Gradient
Идея: в процессе обучения вектор, в котором мы движемся должен быть инертным.
Т.е. мы должны помнить направление, в котором мы двигались ранее и в случае изменения направления,
мы не полностью шагаем в новом направлении, а действуем следующим образом:
будем умножать уже накопленное значение на коэффициент инерции Y и прибавлять очередную величину, умноженную на (1 - Y).
```
v[n] = Y*v[n-1] + (1 - Y) * x

x - текущее значение градиента
Y - коэффициент инертности, 0-1
```
Далее мы заглядываем вперед по вектору обновления и считаем градиент в точке, в которую мы шагнем по инерции.


### Adagrad — adaptive gradient
Идея в том, чтобы веса, которые часто обновляются (в качестве значений приходят ненулевые значения)
обновлять меньше. Для этого добавляется коэффициент, накапливающий значения обновлений и
приращение обновления делится на этот коэффициент.
https://habr.com/ru/post/318970/


### RMSProp (root mean square propagation)
В Adargad со временем может возникнуть ситуация, что значение в знаменателе выросло слишком сильно и
таким образом парализовало изменения значений весов.
Чтобы избавиться от этого, RMSProp использует усредненный по истории квадрат градиента в знаменателе.


### Adadelta
Отличается от RMSProp тем, что в числитель добавляется стабилизирующий член
(важно: на первом шаге этот член должен иметь ненулевое значение).


### Adam (adaptive moment estimation)
Cочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков.
