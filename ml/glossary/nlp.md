# NLP (Natural language processing)

Обработка текстов предполагает несколько этапов:  
 - токенизация (разбиение текста на отдельные слова)  
 - чистка - удаление stop-слов  
 - приведение токенов к первоначальной форме (стэмминг или лемматайзинг - см. ниже)  
 - преобразование токенов к пригодному для обработки виду (числа, вектора чисел, методы см. ниже)  

**Стэмминг** - отрезание у слов окончаний для приведения к базовой форме. Это довольно грубый метод  

**Лемматайзинг** - приведение к слов к базовой форме на основании контекста, формы, ... Это более основательный подход  
[Немного о лемматизации](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)  

**N-grams** - метод, который приводит строку к набору n-gram. 
Например, 2-gram будет выглядеть:
“Natural Language Processing is essential to Computer Science” => “Natural Language, Language Processing, Processing is, is essential, essential to, to Computer, Computer Science”
3-gram будет выглядеть:
“Natural Language Processing, Language Processing is, Processing is essential, is essential to, essential to Computer, to Computer Science”

## Методы кодирования/представления слов

### One hot encoding(OHE)
- подход, при котором для кодирования создается вектор длиной равной размеру словаря и каждому слову соответстсвует одна единица в векторе. Таким образом, слова кодируются по принципу:  
```python
dictionary = ['computer', 'phone', mouse]
computer = [1, 0, 0]
phone = [0, 1, 0]
mouse = [0, 0, 1]
```

### bag of words(BoW)
Из **OHE** легко можно получить *bag of words(BoW)*. Это будет один вектор, в котором для всех слов встречающихся в тексте будут стоять единицы. Правда, в таком случае мы потеряем информацию о порядке слов и о том, сколько раз слова встретились в тексте. Зато мы легко можем сравнивать такие мешки с помощью косинусной меры.  

### TF-IDF (Term frequency - identity frequency)
*TF* - частотность термина - отношение количества повторений слова в документе к количеству слов в документе  
```
Tf = n / D 
```
*IDF* -  обратная частотность документов, измеряет важность термина.  
```
IDF = log(D / t(D));
D - общее число документов
t(D) - число документов, в которые встретился термин t
```
Таким образом, в соответствие каждому токену ставится некоторое число, которое его характеризует.  

**Обратный индекс (Inverted index)** - матрица, в которой строками являются слова, столбцами - документы, а на пересечении - 0 или 1, в зависимости от того, встечается ли слово в документе.  

### word2vec
Метод преобразования слов в вектор чисел. Обладает свойствами:
 - обучается без учителя  
 - для любых двух токенов можно найти расстояние между ними (косинусная мера)  
 - для семантически связанных токенов расстояние оказывается положительным и стремится к 1  

Результатом работы word2vec будет embedding - вектор, характеризующий семантический смысла данного слова. Стоит отметить, что слова, имеющие схожий семантический смысл (часто встречающиеся в схожем контексте), будут иметь малое косинусное расстояние между векторами.

word2vec имеет два основных подхода для генерации embedding-ов:
 - непрерывный мешок слов (Continuous Bag of Words, CBoW) — это такая модель для получения вложений слов, полученных обучением модели предсказывать слова по контексту
 - Skip-Gram же наоборот — получает вложения слов обучая модель предсказывать контекст по слову

Есть немало альтернатив word2vec-у, но особого выигрыша они не дают: **GloVe**, **AdaGram**, **Text2Vec**, **Seq2Vec**

### FastText
В отличие от word2vec, разбивает еще и слова на n-граммы. FastText же может дать лучший результат разбивая слова на части и, используя векторы этих частей, создать результирующий вектор для слова.  


### Bert
State-of-the-art-модель 2018-года


### GPT2


## Ссылки

## Bert
## XLnet
https://habr.com/ru/company/ods/blog/458928/

Парсинг зависимостей слов в тексте:
 - встроенный в spacy - менее продвинутый
 - Parsey McParseface
 - ParseySaurus

Очень крутая статья про алгоритмы извлечения фактов из текста:
https://proglib.io/p/fun-nlp/?unapproved=3317&moderation-hash=5efa690598d4ed733f8d0859d75736d3#comment-3317
