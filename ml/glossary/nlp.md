# NLP


Парсинг зависимостей слов в тексте:
 - встроенный в spacy - менее продвинутый
 - Parsey McParseface
 - ParseySaurus

Очень крутая статья про алгоритмы извлечения фактов из текста:
https://proglib.io/p/fun-nlp/?unapproved=3317&moderation-hash=5efa690598d4ed733f8d0859d75736d3#comment-3317

**Стэмминг** - отрезание у слов окончаний для приведения к базовой форме. Это довольно грубый метод  

**Лемматайзинг** - приведение к слов к базовой форме на основании контекста, формы, ... Это более основательный подход  
[Немного о лемматизации](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)  

**N-grams** - метод, который приводит строку к набору n-gram. 
Например, 2-gram будет выглядеть:
“Natural Language Processing is essential to Computer Science” => “Natural Language, Language Processing, Processing is, is essential, essential to, to Computer, Computer Science”
3-gram будет выглядеть:
“Natural Language Processing, Language Processing is, Processing is essential, is essential to, essential to Computer, to Computer Science”

**Embedding** - статистический метод, который позволяет предсказать вероятность слова по его окружению. Процесс тренировки следующий:
 - каждому слову соответствует вектор длини k (k - размер контекста)
 - берется текст, в котором в центре находится интересующее нас слово, количество слов по бокам равно k
 - далее по специальной формуле корректируем наш вектор
в основе лежит гипотеза локальности: "слова, которые встречаются в одинаковых окружениях, имеют близкие значения"

## Методы кодирования/представления слов

**One hot encoding(OHE)** - подход, при котором для кодирования создается вектор длиной равной размеру словаря и каждому слову соответстсвует одна единица в векторе.
Таким образом слова кодируются по принципу:
```python
dictionary = ['computer', 'phone', mouse]
computer = [1, 0, 0]
phone = [0, 1, 0]
mouse = [0, 0, 1]
```

### TF-IDF
TF - сколько раз встретилось слово в текущем документе
IDF = N/D; N - число документов, в которых встретилось слово, D - общее число документов

из OHE легко можно получить **bag of words(BoW)**. Это будет один вектор, в котором для всех слов встречающихся в тексте будут стоять единицы.
Правда, в таком случае мы потеряем информацию о порядке слов и о том, сколько раз слова встретились в тексте.
Зато мы легко можем сравнивать такие мешки с помощью косинусной меры.

**Обратный индекс (Inverted index)** - матрица, в которой строками являются слова, столбцами - документы, а на пересечении - 0 или 1, в зависимости от того, встечается ли слово в документе.  

автоматическая генерация тестов из текстов

**word2vec** - метод преобразования слов в вектор чисел. Обладает свойствами:
 - обучается без учителя
 - для любых двух токенов можно найти расстояние между ними (косинусная мера)
 - для семантически связанных токенов расстояние оказывается положительным и стремится к 1
 - взаимозаменяемые слова (модель Skipgram);
 - ассоциированные слова (модель BagofWords).
Есть немало альтернативных методик, но особого выигрыша они не дают: GloVe, AdaGram, Text2Vec, Seq2Vec.

GloVe

### Conditional Random Fields (CRF) - условные случайные поля
### Скрытые марковские модели
### RNNSharp

## Посмотреть:
https://explosion.ai/blog/parsing-english-in-python

## Bert
## XLnet
https://habr.com/ru/company/ods/blog/458928/
