# Классификационные модели

## Logistic Regression
Логистическая регрессия — это статистическая модель, используемая для прогнозирования вероятности возникновения некоторого события путём подгонки данных к логистической кривой  
Для логистической регресси характерно то, что выходные значения равны: [0, 1]  
А функция вероятности принадлежности к одному из классов представляет собой логистическую функцию - сигмойд  
```python
  f(x) = 1 / (1 + e^-x)
```


## SVM (support vector machine)
Метод опорных векторов
Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве  
Требует нормировки данных  
Мы строим гиперплоскость, резделяющую данные на два класса, а затем еще две гиперплоскости
по бокам и стараемся максимизировать расстояние между ними  
[Описание](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2)


## Decision Trees
[см decision trees](./decision_trees.md)


# Методы кластеризации

## KMeans
Алгоритм, стремящийся минимизировать суммарное квадратичное отклонение точек кластеров от центров этих кластеров.
Центроиды - центры кластеров.
На первоначальном этапе выбираются либо случайные координаты центроидов, либо по определенному правилу.
Затем на каждом этапе происходит подсчет евклидова расстояния для каждого признака к каждому центроиду и выбор минимального (классификация).
Затем центроид для каждого кластера перевычисляет свои координаты  
Алгоритм останавливается когда координаты центроид перестают меняться более, чем на заданную m величину.  
Недостатки:  
 - число кластеров разбивки необходимо знать заранее  
 - не гарантируется достижение глобального минимума суммарного квадратичного отклонения V, а только одного из локальных минимумов  
 - результат зависит от выбора исходных центров кластеров, их оптимальный выбор неизвестен  


## HDBSCAN
Метод основан на плотности данных. Необходимо задать минимальное пороговое значение
для образования кластера.
(является улучшенной версией DBSCAN)
Преимущества:
 - не требует задания количества кластеров при старте обуяения  
 - может определять отдельные данные как шум
 
 
 # Деревья принятия решений

Основная идея - максимальное уменьшение энтропии после прохождения каждого предиката  
Для расчета энтропии используется одна из формул:

## [Энтропия Шеннона](#shennon-entropy)
```python
S = -SUM(p[i] * log2(p[i]))
```
p[i] - вероятность нахождения системы в i-ом состоянии

## [Энтропия Джинни](#ginni-entropy)
```python
G = 1 - SUM(p[i]^2)
```

Способы борьбы с переобучением:
 - стрижка  
 - ограничение глубины  

## RandomForest (случайный лес)
Усреднение ответов деревьев построенных до максимальной глубины  

Преимущества:
 - интерпретируемость и визуализируемость  
 - быстрый процесс обучения и прогнозирования  
 - малое число параметров модели  
 - поддержка числовых и категориальных признаков  

Недостатки:
 - высокая чувствительность к шумам в данных  
 - дерево умееть только интерполировать, но не экстраполировать  



# Методы понижения размерности

## PCA(Principal component analysis)
Метод главных компонент - метод уменьшения размерности данных.
Все компоненты подбираются так, чтобы у них была максимальная дисперсия и минимальная корреляция с другими главными компонентами  
[Хорошее описание](https://habr.com/ru/company/ods/blog/325654/)  
Суть в том, что если у нас есть несколько признаков и дисперсия одного сильно отличается от
дисперсии другого, то это значит, что мы можем понизить размерность нашей выборки  
Для того, чтобы понять какое число главных компонент следует оставить, можно посмотреть на
[график зависимости общей доли объясняемой дисперсии от числа главных компонент](https://github.com/Yorko/mlcourse.ai/blob/master/jupyter_russian/topic07_unsupervised/lesson7_part1_PCA.ipynb)  


## TSNE (t-distributed stochastic neighbor embedding)
Стохастическое вложение соседей с t-распределением.  
Данный алгоритм позволяет биективно спроецировать точки многомерного пространства на пространство малой размерности (2 или 3).
Алгоритм основан на случайных изначальных значениях, поэтому при каждом запуске будут получаться различные результаты
[Отличное описание работы алгоритма](http://datareview.info/article/algoritm-t-sne-illyustrirovannyiy-vvodnyiy-kurs/)  
