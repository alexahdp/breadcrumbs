# Регрессионные модели  

## Linear Regression
Линейная регрессия - метод восстановления зависимости между двумя переменными  
Суть метода заключается в подборе векторов коэффициентов W для уравнения: Y = WX.  

Для использования линейной регрессии необходимо соблюдение двух обязательных условий:
 - постоянная дисперсия для всей выборки  
 - отсутствие автокорреляции случайных ошибок  
для оценки качества модели стоит использовать **SSE**  
Также стоит проверять значение числа обусловленности для диагностирования линейной зависимости
между аргументами  

За счет простой формы быстро и легко обучаются, популярны при работе с большими объемами данных  
Для реальных прогнозов и упреждающего анализа линейная регрессия не очень подходит  
Может очень хорошо работать на тренировочных данных и плохо на тестовых, т.е. модели линейной регрессии склонны к переобучению  

Коэффициенты W подбираются по формуле:
```python
W = (Xt*X)^-1 * Xt*y
          (1 x1)
, где X = (1 x2) (это одна матрица)
          (1 x3)
Xt - транспонированная
X^-1 - обратная матрица
```

## KNN (K nearest neighbors)
Метод k-средних - алгоритм используемый как для классификации, так и для регрессии  
В случае классификации объекту будет назначен класс как у k ближайших соседей  
В случае же регрессии объекту присваивается среднее значение k ближайших соседей  
Для данного алгоритма обычно необходимо проводить нормировку данных, поскольку используется
дистанция, а сильно отличающиеся по порядкам параметры могут начать сильно влиять на результат  
Применение:
 - часто используется для построения meta-признаков  
 - рекомендательные системы  

Преимущества:
 - простая реализация  
 - можно гибко использовать, задавая кастомные метрики  
 - неплохая интерпретация  

Недостатки
 - на больших объемах может работать довольно медленно  
 - при большом количестве признаков сложно отобрать значимые  
 - нет теоретических оснований выбора определенного числа соседей — только перебор  
 - в случае малого числа соседей метод чувствителен к выбросам, то есть склонен переобучаться  
 - как правило, плохо работает, когда признаков много, из-за "прояклятия размерности"  


## Decision Tree
В принципе, можно использовать деревья для построения регрессионных моделей  
[см decision trees](./decision_trees.md)


## Lasso regression
(L1-регуляризация)
Cходна с гребневой, за исключением того, что коэффициенты регрессии могут равняться нулю (часть признаков при этом исключается из модели)  


## Ridge regression(гребневая)
(L2-регуляризация)
Усовершенствование линейной регрессии с повышенной устойчивостью к ошибкам,
налагающая ограничения на коэффициенты регрессии для получения куда более
приближенного к реальности результата.  

В отличие от L1-регуляризации добавляется penalty (наказание) к функции потерь  
Часто применяется для понижения размерности, поскольку позволяет избавиться от наименее значимых аргументов.  
Когда стоит использовать:
 - при сильной обусловленности признаков  
 - сильно различаются собственные значения или некоторые из них близки к нулю  
 - в аргументах есть сильно скоррелированные значения  

Ридж-регрессия или гребневая регрессия предполагает оценку параметров по следующей формуле:
```python
W = (Xt*X + LI)^-1 * Xt*y
          (1 x1)
, где X = (1 x2) (это одна матрица)
          (1 x3)
Xt - транспонированная
X^-1 - обратная матрица
L - это коэффициент регуляризации, то, насколько сильно мы хотим учитывать условие I
```

## Байесовская регрессия
Похожа на гребневую регрессию, однако основана на том допущении,
что в данных шум (ошибка) распределен нормально  

## Links
[10 типов регрессии](http://datareview.info/article/10-tipov-regressii-kakoy-vyibrat/)
[Базовые принципы машинного обучения на примере линейной регрессии](https://habr.com/ru/company/ods/blog/322076/)


# Классификационные модели

## Logistic Regression
Логистическая регрессия — это статистическая модель, используемая для прогнозирования вероятности возникновения некоторого события путём подгонки данных к логистической кривой  
Для логистической регресси характерно то, что выходные значения равны: [0, 1]  
А функция вероятности принадлежности к одному из классов представляет собой логистическую функцию - сигмойд  
```python
  f(x) = 1 / (1 + e^-x)
```


## SVM (support vector machine)
Метод опорных векторов
Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве  
Требует нормировки данных  
Мы строим гиперплоскость, резделяющую данные на два класса, а затем еще две гиперплоскости
по бокам и стараемся максимизировать расстояние между ними  
[Описание](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2)


## Decision Trees
[см decision trees](./decision_trees.md)


# Методы кластеризации

## KMeans
Алгоритм, стремящийся минимизировать суммарное квадратичное отклонение точек кластеров от центров этих кластеров.
Центроиды - центры кластеров.
На первоначальном этапе выбираются либо случайные координаты центроидов, либо по определенному правилу.
Затем на каждом этапе происходит подсчет евклидова расстояния для каждого признака к каждому центроиду и выбор минимального (классификация).
Затем центроид для каждого кластера перевычисляет свои координаты  
Алгоритм останавливается когда координаты центроид перестают меняться более, чем на заданную m величину.  
Недостатки:  
 - число кластеров разбивки необходимо знать заранее  
 - не гарантируется достижение глобального минимума суммарного квадратичного отклонения V, а только одного из локальных минимумов  
 - результат зависит от выбора исходных центров кластеров, их оптимальный выбор неизвестен  


## HDBSCAN
Метод основан на плотности данных. Необходимо задать минимальное пороговое значение
для образования кластера.
(является улучшенной версией DBSCAN)
Преимущества:
 - не требует задания количества кластеров при старте обуяения  
 - может определять отдельные данные как шум
 
 
 # Деревья принятия решений

Основная идея - максимальное уменьшение энтропии после прохождения каждого предиката  
Для расчета энтропии используется одна из формул:

## [Энтропия Шеннона](#shennon-entropy)
```python
S = -SUM(p[i] * log2(p[i]))
```
p[i] - вероятность нахождения системы в i-ом состоянии

## [Энтропия Джинни](#ginni-entropy)
```python
G = 1 - SUM(p[i]^2)
```

Способы борьбы с переобучением:
 - стрижка  
 - ограничение глубины  

## RandomForest (случайный лес)
Усреднение ответов деревьев построенных до максимальной глубины  

Преимущества:
 - интерпретируемость и визуализируемость  
 - быстрый процесс обучения и прогнозирования  
 - малое число параметров модели  
 - поддержка числовых и категориальных признаков  

Недостатки:
 - высокая чувствительность к шумам в данных  
 - дерево умееть только интерполировать, но не экстраполировать  



# Методы понижения размерности

## PCA(Principal component analysis)
Метод главных компонент - метод уменьшения размерности данных.
Все компоненты подбираются так, чтобы у них была максимальная дисперсия и минимальная корреляция с другими главными компонентами  
[Хорошее описание](https://habr.com/ru/company/ods/blog/325654/)  
Суть в том, что если у нас есть несколько признаков и дисперсия одного сильно отличается от
дисперсии другого, то это значит, что мы можем понизить размерность нашей выборки  
Для того, чтобы понять какое число главных компонент следует оставить, можно посмотреть на
[график зависимости общей доли объясняемой дисперсии от числа главных компонент](https://github.com/Yorko/mlcourse.ai/blob/master/jupyter_russian/topic07_unsupervised/lesson7_part1_PCA.ipynb)  


## TSNE (t-distributed stochastic neighbor embedding)
Стохастическое вложение соседей с t-распределением.  
Данный алгоритм позволяет биективно спроецировать точки многомерного пространства на пространство малой размерности (2 или 3).
Алгоритм основан на случайных изначальных значениях, поэтому при каждом запуске будут получаться различные результаты
[Отличное описание работы алгоритма](http://datareview.info/article/algoritm-t-sne-illyustrirovannyiy-vvodnyiy-kurs/)  
