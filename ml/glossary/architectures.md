# Наиболее известные архитектуры
https://habr.com/ru/company/nixsolutions/blog/430524/

## LeNet (1994)
По сути, первая сверточная нейронная сеть. В ней было всего 3 слоя: convolution, pooling, non-lenearity.

## AlexNet (2012)
![AlexNet architecture](https://neurohive.io/wp-content/uploads/2018/10/Capture-12-1.jpg)
 - был использован Relu вместо нелинейной функции активации  - это сильно ускорило обучение  
 - была использована методика dropout  - позволяет уменьшить переобучение, но увеличивает время обучения  
 - перекрытие maxPooling, что позволяет избежать эффектов усреднения (average pooling)  
 - имеет большее количество фильтров на слое
 - используется SGD
Размеры фильтров - 9x9, 11x11  
Таким образом, AlexNet содержит 5 сверточных слоев и 3 полносвязных слоя.
Relu применяется после каждого сверточного и полносвязного слоя.
Дропаут применяется перед первым и вторым полносвязными слоями.
 
## VGG16 (2014)
![VGG16 architecture](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network-1-e1542973058418.jpg)
![VGG16 scheme](https://neurohive.io/wp-content/uploads/2018/11/vgg16-2.png)
Архитектура является улучшенной версией AlexNet.  
 - вместо фильтров 11x11, 9x9, 5x5 используются более мелкие фильтры (3x3) идущие друг за другом.
 Объединение таких фильтров в последовательность дало отличный результат  
 позволило эмулировать более крупные рецептивные поля  
 - во многих слоях используется большое количество свойств, поэтому обучение требовало больших вычислительных затрат  
Недостатки:
 - очень медленная скорость обучения  
 - очень много весит сама архитектура сети  
Модель показывает лучшие результаты по сравнению с предшественниками.
Было показано, что глубина представления положительно влияет на точность классификации.  

## NiN - network-in-network (2014)
 - использование сверток 1x1 (mlconv) - применяются для пространственного комбинирования свойств после свертки в рамках карт свойств

## GoogleNet/Inception (2014)
 - использовали NiN-блоки для уменьшения количества свойств  

## GoogleNet/Inception v2, v3 (2015)
 - представлена архитектура batch-normalization-inception
 - По мере возможности используются только свёртки 3x3  

## ResNet (2015)
 - простая, но революционная идея - подаём выходные данные двух успешных свёрточных слоёв 
и обходим входные данные для следующего слоя
 - был продемонстрирован пример обучения сети из нескольких сотен и даже тысячи слоев  

## Inception v4
 - 

## Enet

## FractalNet
