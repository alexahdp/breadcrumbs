# Boosting
Идея бустинга состоит в том, чтобы использовать ансамбли более слабых моделей, которые строятся в ходе процесса итеративного обучения. Недостатком бустинга является то, что в ходе такого обучения строится очень большое количество моделей, что несколько затрудняет интерпретацию результатов, создает дополнительные требования к железу.   

### GBM (Градиентный бустинг)
Использует идею: на каждом шаге обучения корректировать те модели, которые выдали наиболее далекий от верного ответ  
Для работы бустинга необходимо:
 - выбрать функцию потерь
 - выбрать семейство функций на которых будет обучаться модель
 - выбрать дополнительные гиперпараметры (например, глубина деревьев)

Существует несколько форм градиентного бустинга:
 * стандартный  
 * стохастический  
 * регуляризованный (c L1 и L2 регуляризацией)  


### AdaBoost (adaptive boosting)
Имеет целью объединение нескольких слабых классификаторов в один сильный. 
Недостаток: чувствителен к шумам в данных и выбросам.  
На данный момент практически не используется из-за наличия более эффективных методов.  
[Ссылка](https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c)  


### XGBoost
На каждой итерации вычисляется ошибка и в ансамбль добавляется модель(дерево), которое будет компенсировать эту ошибку. Таким образом, на каждом шаге уменьшается средняя ошибка предсказания совокупности моделей. Новые модели добавляются в ансамбль до тех пор, пока ошибка уменьшается или пока не сработает правило ранней остановки.


### LightGBM
LightGBM основан на принципе GOSS(Gradient Based One Side Sampling). Суть в том, что модель хорошо обучается на примерах с малыми градиентами. Поэтому в GOSS выполняется следующее:
 - входные данные сортируются по убыванию в соответствии со значениями их градиентов  
 - берутся первые n значений сверху  
 - берется часть оставшихся значений и случайным образом перемешивается с выбранными не предыдущем этапе данными  
 - происходит обучение и корректировка весов  

Преимущества:
 * большая скорость обучения и большая эффективность  
 * меньшее потребление памяти  
 * хорошая точность  
 * возможность обработки больших датасетов  

 
### CatBoost
0) предподготовка данных автоматическое приведение категориальных признаков, типов данных, ...
1) перестановка данных случайным образом, подсчет счетчиков категориальных признаков
2) жадный подбор уровней дерева на основе перестановок в данных
3) сторится оценка того, как изменится целевая функция и на основании этого принимается решение
 о том, какое разбиение дерева использовать и по каким признакам
4) повторяем пункты 1-3 пока ошибка уменьшается


###
(TODO)
Существуют различные алгоритмы для обучения моделей деревьев: CART, C4.5, CHAID.


### Ссылки
[XGBoost](http://neerc.ifmo.ru/wiki/index.php?title=XGBoost)  
[What makes LightGBM lightning fast?](https://medium.com/@abhisheksharma_57055/what-makes-lightgbm-lightning-fast-a27cf0d9785e)  
[Принцип работы CatBoost](https://habr.com/ru/company/yandex/blog/458790/)  
